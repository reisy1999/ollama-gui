# READMEğŸš€

## purpose
ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰åˆå­¦è€…ã®ç§ãŒã€ã‚¢ã‚¸ãƒ£ã‚¤ãƒ«é–‹ç™ºã§å°‘ã—ãšã¤ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚’è‚²ã¦ã¦ã€
ç§ã ã‘ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚

## Overview

ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œã™ã‚‹ **Ollama API** ã‚’ã€**Next.js ã® API Route çµŒç”±**ã§åˆ©ç”¨ã™ã‚‹ Web UI ã§ã™ã€‚

* Framework: **Next.js (App Router)**
* Language: **TypeScript**
* LLM Backend: **Ollama (local)**

## Requirements

* Node.js >= 18
* Ollama(ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)

## Setup

### 1. Ollama ã‚’èµ·å‹•

```bash
ollama serve
ollama pull llama3
```

### 2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆèµ·å‹•

```bash
git clone <REPOSITORY_URL>
cd <PROJECT_DIR>
npm install
npm run dev
```

ãƒ–ãƒ©ã‚¦ã‚¶:

* [http://localhost:3000](http://localhost:3000)

## Environment Variables

`.env.local` ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

```env
# Ollama API
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=__YOUR_MODEL_NAME__
```

## Architecture

```
Browser
  â”‚
  â”‚ POST /api/chat
  â–¼
Next.js API Route
  â”‚
  â”‚ POST http://localhost:11434/api/chat
  â–¼
Ollama (local)
```
